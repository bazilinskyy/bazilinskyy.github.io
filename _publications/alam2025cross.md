---
layout: publication
sitemap: false
title: "Cross or Nah? LLMs Get in the mindset of a pedestrian in front of automated car with an eHMI"
authors: Alam, M. S., Bazilinskyy, P.
pdf: alam2025cross
image: alam2025cross.jpg
display: Submitted for publication.
year: 
doi: 
code: https://github.com/Shaadalam9/llms-av-crowdsourced
suppmat: https://www.dropbox.com/scl/fo/xs37ldfp72dspsrjykc2d/AEnstqB2KFDRjnnl8M0VJz8?rlkey=63vcekw3qr2c91ao38j1wtxow
abstract: "This study examines the effectiveness of using large language model-based personas to evaluate external human-machine interfaces (eHMIs) in automated vehicles. Various models including miniCPM-V, LLaVA, LLaVA-LLaMA-3, Llama3.2 vision, Moondream, BakLLaVA, Granite3.2 vision, LLaVA-Phi3, Gemma 3, Deepseek-vl2, and ChatGPT-4o were used to simulate pedestrian perspectives. Models assessed vehicle images with eHMIs, assigning scores from 0 (completely unwilling) to 100 (fully confident) regarding crossing decisions. Each model was compared with 15 trials with randomised image sequences, both with and without prior chat context, and the results were compared with crowdsourced human ratings. The findings indicate Gemma3: 27B performed better without chat history (r = 0.85), while ChatGPT-4o was superior when the historical context was included (r = 0.81). In contrast, models such as Deepseek-vl2 and BakLLaVA provided uniform confidence scores with memory context, while Llama3.2 vision failed entirely to produce outputs."
---